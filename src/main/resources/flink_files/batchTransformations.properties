Map = Takes one element and produces one element.
#取一个元素并生成一个元素
data.map { x => x.toInt }

FlatMap = Takes one element and produces zero, one, or more elements.
#取一个元素，生成0 1 或者多个元素，其实说实话是flat的作用是将所有生成的元素全部都看做独立的个体！！！
data.flatMap { str => str.split(" ") }

MapPartition = Transforms a parallel partition in a single function call. The function get the partition as an `Iterator` and can produce an arbitrary number of result values. The number of elements in each partition depends on the degree-of-parallelism and previous operations.
#这里的mappartition和想想中的有些区别，本以为元素是一个迭代器，实际上还是一个dataset
data.mapPartition { in => in map { (_, 1) } }

Filter = Evaluates a boolean function for each element and retains those for which the function returns true.
#IMPORTANT: The system assumes that the function does not modify the element on which the predicate is applied. Violating this assumption can lead to incorrect results.
#不必多说，就是一个判断过程
data.filter { _ > 1000 }

Reduce = Combines a group of elements into a single element by repeatedly combining two elements into one. Reduce may be applied on a full data set, or on a grouped data set.
#不必多说，一定要注意输出的格式，已经错了很多次了(x ,y) => (x._1, x._2 + y._2)
data.reduce { _ + _ }

ReduceGroup = Combines a group of elements into one or more elements. ReduceGroup may be applied on a full data set, or on a grouped data set.
#没用明白，其实永别的代替完全没问题
data.reduceGroup { elements => elements.sum }

Aggregate = Aggregates a group of values into a single value. Aggregation functions can be thought of as built-in reduce functions. Aggregate may be applied on a full data set, or on a grouped data set.
#聚合函数，聚合函数有很多,其实有两种方式调用，第一种就是下面的方式比较麻烦，另一种则是再下面直接调用sum() min() max() 函数
#val input: DataSet[(Int, String, Double)] = // [...]
#val output: DataSet[(Int, String, Double)] = input.aggregate(SUM, 0).aggregate(MIN, 2)
#You can also use short-hand syntax for minimum, maximum, and sum aggregations.
#val input: DataSet[(Int, String, Double)] = // [...]
#val output: DataSet[(Int, String, Double)] = input.sum(0).min(2)

Distinct = Returns the distinct elements of a data set. It removes the duplicate entries from the input DataSet, with respect to all fields of the elements, or a subset of fields.
#去重，可以根据全部字段去重，也可以根据部分字段去重，可以理解
data.distinct()

Join = Joins two data sets by creating all pairs of elements that are equal on their keys. Optionally uses a JoinFunction to turn the pair of elements into a single element, or a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements. See the keys section to learn how to define join keys.

#// In this case tuple fields are used as keys. "0" is the join field on the first tuple
#// "1" is the join field on the second tuple.
#val result = input1.join(input2).where(0).equalTo(1)
You can specify the way that the runtime executes the join via Join Hints. The hints describe whether the join happens through partitioning or broadcasting, and whether it uses a sort-based or a hash-based algorithm. Please refer to the Transformations Guide for a list of possible hints and an example.
If no hint is specified, the system will try to make an estimate of the input sizes and pick the best strategy according to those estimates.
#// This executes a join by broadcasting the first data set
#// using a hash table for the broadcast data
#val result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST)
#.where(0).equalTo(1)
#Note that the join transformation works only for equi-joins. Other join types need to be expressed using OuterJoin or CoGroup.
#可以不指定join策略，系统会选择最优的方式

OuterJoin = Performs a left, right, or full outer join on two data sets. Outer joins are similar to regular (inner) joins and create all pairs of elements that are equal on their keys. In addition, records of the "outer" side (left, right, or both in case of full) are preserved if no matching key is found in the other side. Matching pairs of elements (or one element and a `null` value for the other input) are given to a JoinFunction to turn the pair of elements into a single element, or to a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements. See the keys section to learn how to define join keys.
#val joined = left.leftOuterJoin(right).where(0).equalTo(1) {
#(left, right) =>
#val a = if (left == null) "none" else left._1
#(a, right)
#}

CoGroup = The two-dimensional variant of the reduce operation. Groups each input on one or more fields and then joins the groups. The transformation function is called per pair of groups. See the keys section to learn how to define coGroup keys.
#已经是第三次说这个东西了，与alljoin类似，但是返回的结果的value是一个集合
data1.coGroup(data2).where(0).equalTo(1)

Cross = Builds the Cartesian product (cross product) of two inputs, creating all pairs of elements. Optionally uses a CrossFunction to turn the pair of elements into a single element
#笛卡尔积的合并，其实可以认为是结合，因为结合没有任何条件
#val data1: DataSet[Int] = // [...]
#val data2: DataSet[String] = // [...]
#val result: DataSet[(Int, String)] = data1.cross(data2)
#Note: Cross is potentially a very compute-intensive operation which can challenge even large compute clusters! It is advised to hint the system with the DataSet sizes by using crossWithTiny() and crossWithHuge().

Union = Produces the union of two data sets.
#普通的合并，会有重复数据
data.union(data2)

Rebalance = Evenly rebalances the parallel partitions of a data set to eliminate data skew. Only Map-like transformations may follow a rebalance transformation.
#重新平衡，这个怎么说呢，如果已经明显感觉到数据倾斜的话可以用，但是一般是不会用的
#val data1: DataSet[Int] = // [...]
#val result: DataSet[(Int, String)] = data1.rebalance().map(...)

Hash-Partition = Hash-partitions a data set on a given key. Keys can be specified as position keys, expression keys, and key selector functions.
#hash 分区
#val in: DataSet[(Int, String)] = // [...]
#val result = in.partitionByHash(0).mapPartition { ... }

Range-Partition = Range-partitions a data set on a given key. Keys can be specified as position keys, expression keys, and key selector functions.
#range 范围分区
#val in: DataSet[(Int, String)] = // [...]
#val result = in.partitionByRange(0).mapPartition { ... }

Custom Partitioning = Assigns records based on a key to a specific partition using a custom Partitioner function. The key can be specified as position key, expression key, and key selector function. 
Note: This method only works with a single field key.
#自定义分区函数
#val in: DataSet[(Int, String)] = // [...]
#val result = in.partitionCustom(partitioner, key).mapPartition { ... }

Sort Partition = Locally sorts all partitions of a data set on a specified field in a specified order. Fields can be specified as tuple positions or field expressions. Sorting on multiple fields is done by chaining sortPartition() calls.
#根据排序分区
#val in: DataSet[(Int, String)] = // [...]
#val result = in.sortPartition(1, Order.ASCENDING).mapPartition { ... }

First-n = Returns the first n (arbitrary) elements of a data set. First-n can be applied on a regular data set, a grouped data set, or a grouped-sorted data set. Grouping keys can be specified as key-selector functions, tuple positions or case class fields.
#取随机n个元素作为返回值，当然也需要注意一个问题，分不分区返回的数量是不一样的！！！
#val in: DataSet[(Int, String)] = // [...]
#// regular data set
#val result1 = in.first(3)  直接函数3个
#// grouped data set
#val result2 = in.groupBy(0).first(3)  每个分组都返回3个，不够的可以少于3个
#// grouped-sorted data set
#val result3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) 每个分组返回3个，不够的可以少于3个，同时并不是随机返回了，要根据排序的顺序返回元素