reduce(func) = Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.
#聚合，最直接的聚合，一个一个计算，需要给定func

collect() = Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.
#把所有的数据拉回friver，放进array中，适合少量数据的collect，如果为了避免oom，可以rdd.take(n)

count() = Return the number of elements in the dataset.
#计数

first() = Return the first element of the dataset (similar to take(1)).
#去第一个元素返回，类似于take(1)

take(n) = Return an array with the first n elements of the dataset.
#取前几个作为array返回

takeSample(withReplacement, num, [seed]) = Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.
#自从了解了sample函数之后就知道这个函数的意义了，只不过这个takesample函数是action，会直接执行
#随机取样num个元素，replacement是是否为放回采样的标识

takeOrdered(n, [ordering]) = Return the first n elements of the RDD using either their natural order or a custom comparator.
#取前n个元素，但是和take的区别在于是否是排好序的取从小到大的前n个而top(n)是从大到小去前topN

saveAsTextFile(path) = Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.
#写入一个或多个文本文件，只需要给定目录,并且该目录必须是不存在的
#文本文件里面保存的就是可以直观看出来的数据

saveAsSequenceFile(path) = (Java and Scala)	Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).
#写入一个或者多个sequence文件,但是要求必须是K，V格式，不然不能存为sequence文件

saveAsObjectFile(path) = (Java and Scala)	Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().
#类似于java中的序列化，将对象进行序列化，可以用objextFile()读取

countByKey() = Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.
#根据key计数、真的kv形式的计数
#countByValue()计数，根据kv对象计数

foreach(func) = Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.
#foreachPartition()来代替，效率会高
#Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.
#关于这个共享变量的问题有三个解决办法
#最笨的，消耗资源最多的：全局的volitile变量
#其次：广播变量scsc.broadcast(变量)
#全剧计数：val nErrors=sc.accumulator(0.0)  foreach（对nErrors进行计数）