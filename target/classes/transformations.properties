map(func) = Return a new distributed dataset formed by passing each element of the source through a function func.
#返回一个新的rdd，对每一个元素执行map内的func函数

filter(func) = Return a new dataset formed by selecting those elements of the source on which func returns true.
#返回一个新的rdd，应用于每个元素func并返回true的元素，过滤

flatMap(func) = Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).
#和map类似，但是每个输入的item在应用map的时候输出也是一个元素，但是flatmap每个item返回0或者多个items，也就是说每个item应用func之后返回
#的是一个seq序列，通俗来讲叫做拉平map
#其实一句话解释区别：：：map的结果是Array中的每个元素是以行为单位的，但是flatmap的结果是Array中的每个元素是map的结果的每个元素放在一起
#举个例子lines.map(line => line.split(" "))的结果是Array[Array[String]] 但是lines.flatMap(line => line.split(" "))的结果是Array[String]。

mapPartitions(func) = Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.
#与map相似，但是对rdd的每个partition分开执行的，因为partition是Iterator，所以func函数接收的参数也应该是Iterator<T>,
#

mapPartitionsWithIndex(func) = Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.
#与mappartitions非常相似，区别在于，每个分区的iterator都携带一个index（分区索引）
#注意mapPartitions和mapPartitonsWithIndex这两个的func的返回值需要是一个Iterator，与foreachPartition不同，foreachPartition是action。而map....是记录操作结果还是rdd

sample(withReplacement, fraction, seed) = Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.
#指随机抽样，seed是随机数种子，可以理解，withreplacement的是意思是不是有放回的随机抽样！！

union(otherDataset) = Return a new dataset that contains the union of the elements in the source dataset and the argument.
#合并数据集，但会一个新的rdd包含两个rdd的所有，不去重

intersection(otherDataset) = Return a new RDD that contains the intersection of elements in the source dataset and the argument.
#取交集

distinct([numPartitions])) = Return a new dataset that contains the distinct elements of the source dataset.
#去重，返回的是一个新的数据集

groupByKey([numPartitions]) = When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. 
#这段英文解释的非常好，如果一个数据集是k,v形式，每个元素都是kv，调用groupByKey之后则根据key分组，同样的是K,V形式，但是K是唯一的，V是一个iterator迭代器
#Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.
#这句话的意思是如果不仅仅是分组还需要计算每一个分组的值，那么最好用reduceByKey和aggregateByKey,因为后者会在map任务的时候就进行reduce，防止大量的shuffle占用网络io
#注意：这三个函数都可以指定numpartitions数量，这个数量是reducer task的数量，其实也就是结果rdd分区的数量
#Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks.

reduceByKey(func, [numPartitions]) = When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.
#分组聚合

aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions]) = When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.
#和reducebykey的区别在于zerovalue，是一个高阶函数，每个map中分组后对每个元素进行swqOp函数，基数是zerovalue，然后将每个分区（map的结果）在给到reducer
#再对已经分完组并执行完seqOP的元素再次进行分组聚合....说白了reducebykey就是调用的combinebykey，而aggregatebykey则是最后一步调用了combinebykey
#所以如果逻辑比较复杂的时候、有基数的时候就用aggregatebykey，一般的聚合就是reducebykey

sortByKey([ascending], [numPartitions]) = When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.
#根据key排序，要求k实现了ordered函数

join(otherDataset, [numPartitions]) = When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.
#join：两个rdd都有的元素才进行join
#leftouterjoin：左边有的全部join
#rightouterjoin：邮编全有的进行join
#没有的都为None

cogroup(otherDataset, [numPartitions]) = When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.
#知道join的原理之后cogroup就比较好理解，join是俩者都有的才进行join，并且V是一个数组，而cogroup是多有都会出现
#先进行自己rdd中的分组然后再与另外的rdd进行join，V的形式是一个元祖，分别代表两个rdd分组的结果
#(1,(CompactBuffer(a, c),CompactBuffer())),(2,(CompactBuffer(b),CompactBuffer(a, f))),(3,(CompactBuffer(),CompactBuffer(c)))

cartesian(otherDataset) = When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).
#全组合，结果是n*m个元祖，合并成一个数组作为新的rdd，每个元素是一个元祖，每个元素的两个元素分别是两个rdd中的元素

pipe(command, [envVars]) = Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.
#调用外部程序给定一个shell脚本，执行结果作为新的rdd的数据，用的不多，因为可移植性、拓展性不好

coalesce(numPartitions) = Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.
#降低分区数，降低并行度！适用于大量数据被过滤掉，提高计算速度
#和repartition的区别在于coalesce是窄依赖，可以shuffle也可以不shuffle，而repartition调用的是coalesce(..., shuffle=true)是shuffle的

repartition(numPartitions) = Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.
#随机shuffle

repartitionAndSortWithinPartitions(partitioner) = Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery.
#根据partitioner进行分区并对每个分区结果进行排序，如果需要shuffle并且排序，首选这个repartitionAndSortWithInPatitions而不是调用repartition之后在调用sortby
