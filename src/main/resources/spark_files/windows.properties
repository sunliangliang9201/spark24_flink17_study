window(windowLength, slideInterval) = Return a new DStream which is computed based on windowed batches of the source DStream.
#返回一个新的DStream，包含windowLength长度的RDDS的，slideInterval的频率

countByWindow(windowLength, slideInterval) = Return a sliding window count of elements in the stream.
#返回一个count数，该count是计算window内的计数

reduceByWindow(func, windowLength, slideInterval) = Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using func. The function should be associative and commutative so that it can be computed correctly in parallel.
#返回一个聚合数，同样式计算window内的数，间隔频率仍然是slideInterval

reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]) = When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.
#跟reduceByKWindow类似，但是区别在于reduceByKeyAndWindow应用在(k,v)的元素上，返回的也是(k,v)v的值时经过func计算而来的


#reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) = A more efficient version of the above reduceByKeyAndWindow() where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter invFunc). Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. Note that checkpointing must be enabled for using this operation.
#比reduceByKeyAndWindow更加有效，关键点在于后者每次并不是把整个window长度的rdds应用reduce函数，而是通过前一个window和当前window的交集加上新的数据而得来

countByValueAndWindow(windowLength, slideInterval, [numTasks]) = When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument.
#同样的与countByWindow区别是应用于(k,v)，而且非常类似于reduceByKeyAndWindow，返回的是(k,v)而countByWindow返回的是一个count一个数，并且跟reduceByKeyAndWindow也有点小区别在于（k,v）的v，countByAndWindow的返回v是一个long，计数的值，而reduceByKeyAndWindow的是经过func计算而来的....