#####第一部分：应用在不同的datastream上的transformation，用于日常算子计算#####

Map = DataStream → DataStream	
#Takes one element and produces one element. A map function that doubles the values of the input stream:
#取一个元素，操作，输出一个元素
dataStream.map { x => x * 2 }

FlatMap = DataStream → DataStream	
#Takes one element and produces zero, one, or more elements. A flatmap function that splits sentences to words:
#取一个元素，输出0 1 或者多个元素，可以理解为拉平，如果不用flatmap而是用map的话，输出的应该是一个数组而不是多个独立的元素
dataStream.flatMap { str => str.split(" ") }

Filter = DataStream → DataStream	
#Evaluates a boolean function for each element and retains those for which the function returns true. A filter that filters out zero values:
#对一个元素执行function，如果结果为true就保留flase则丢弃
dataStream.filter { _ != 0 }

KeyBy = DataStream → KeyedStream	
#Logically partitions a stream into disjoint partitions, each partition containing elements of the same key. Internally, this is implemented with hash partitioning. See keys on how to specify keys. This transformation returns a KeyedStream.
#从datastream变成keyedstream，会将流中的数据进行分区，是根据key进行的hash分区（当然可以自定义partitionfunction）
dataStream.keyBy("someKey") // Key by field "someKey"
dataStream.keyBy(0) // Key by the first element of a Tuple

Reduce = KeyedStream → DataStream	
#A "rolling" reduce on a keyed data stream. Combines the current element with the last reduced value and emits the new value.
#注意啦！！！！！这里非常容易出错，因为这里是stream的操作而不是batch的操作，所以叫做rolling滚动聚合
#比如有数据流如下      ("a", 100), ("b", 200), ("c", 300), ("a", 400))
#按照常理应该是三个结果，根据第一个字段分组聚合(b,200)(c,300)(a,500)
#但是真正的结果应该是：(a,100) (b,200) (c,300) (a,500)   所以要理解流动数据的聚合！！！！当然了如果利用的window那就另说了
#A reduce function that creates a stream of partial sums:
keyedStream.reduce { _ + _ }

Fold = KeyedStream → DataStream	
#A "rolling" fold on a keyed data stream with an initial value. Combines the current element with the last folded value and emits the new value.
#顾名思义：叠在一起，关键点还是在于rolling滚动叠
#A fold function that, when applied on the sequence (1,2,3,4,5), emits the sequence "start-1", "start-1-2", "start-1-2-3", ...
#举个例子吧，直接说的话很难理解，还是上面的数据("a", 100), ("b", 200), ("c", 300), ("a", 400))很久字段0分组，然后input.keyBy(0).fold(100L)((x,y) => x + y._2)
#结果应该是200 300 400 600
#注意第一点：是四个结果而不是三个结果，这是因为是流动数据rolling
#注意第二点:第四个数据为什么是600而不是500，这是因为第一个数据和第四个数据是一个分组的！！！
#val result: DataStream[String] = keyedStream.fold("start")((str, i) => { str + "-" + i })

#Aggregations = KeyedStream → DataStream
#Rolling aggregations on a keyed data stream. The difference between min and minBy is that min returns the minimum value, whereas minBy returns the element that has the minimum value in this field (same for max and maxBy).
#关于，min和minBy的区别先不讨论，感觉区别不大
keyedStream.sum(0)
keyedStream.sum("key")
keyedStream.min(0)
keyedStream.min("key")
keyedStream.max(0)
keyedStream.max("key")
keyedStream.minBy(0)
keyedStream.minBy("key")
keyedStream.maxBy(0)
keyedStream.maxBy("key")

#Window = KeyedStream → WindowedStream
#Windows can be defined on already partitioned KeyedStreams. Windows group the data in each key according to some characteristic (e.g., the data that arrived within the last 5 seconds). See windows for a description of windows.
#窗口函数
dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data

WindowAll = DataStream → AllWindowedStream	
Windows can be defined on regular DataStreams. Windows group all the stream events according to some characteristic (e.g., the data that arrived within the last 5 seconds). See windows for a complete description of windows.
WARNING: This is in many cases a non-parallel transformation. All records will be gathered in one task for the windowAll operator.
#注意区别：如果是windowall的话，性能会受影响
dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data

WindowApply = WindowedStream → DataStream
            AllWindowedStream → DataStream	
#Applies a general function to the window as a whole. Below is a function that manually sums the elements of a window.
Note: If you are using a windowAll transformation, you need to use an AllWindowFunction instead.
windowedStream.apply { WindowFunction }
// applying an AllWindowFunction on non-keyed window stream
allWindowedStream.apply { AllWindowFunction }
#关于窗口的操作接下来会学到

WindowReduce = WindowedStream → DataStream	
#Applies a functional reduce function to the window and returns the reduced value.
#窗口聚合函数
windowedStream.reduce { _ + _ }

WindowFold =WindowedStream → DataStream	
#Applies a functional fold function to the window and returns the folded value. The example function, when applied on the sequence (1,2,3,4,5), folds the sequence into the string "start-1-2-3-4-5":
#窗口叠
#val result: DataStream[String] =
windowedStream.fold("start", (str, i) => { str + "-" + i })

Aggregations on windows = WindowedStream → DataStream	
#Aggregates the contents of a window. The difference between min and minBy is that min returns the minimum value, whereas minBy returns the element that has the minimum value in this field (same for max and maxBy).

windowedStream.sum(0)
windowedStream.sum("key")
windowedStream.min(0)
windowedStream.min("key")
windowedStream.max(0)
windowedStream.max("key")
windowedStream.minBy(0)
windowedStream.minBy("key")
windowedStream.maxBy(0)
windowedStream.maxBy("key")

Union = DataStream* → DataStream	
#Union of two or more data streams creating a new stream containing all the elements from all the streams. Note: If you union a data stream with itself you will get each element twice in the resulting stream.
#合并多个dataStream
dataStream.union(otherStream1, otherStream2, ...)

WindowJoin =DataStream,DataStream → DataStream	
#Join two data streams on a given key and a common window.
#窗口join，为什么一定要有window才能join，因为datastream是无边界数据流，不给窗口的join是不可行的，不然你就用dataset，那个是有边界的
dataStream.join(otherStream).where(<key selector>).equalTo(<key selector>).window(TumblingEventTimeWindows.of(Time.seconds(3))).apply { ... }

Window CoGroup = DataStream,DataStream → DataStream	
#Cogroups two data streams on a given key and a common window.
#学习spark的时候已经知道了coGroup与join非常类似。不过join是两者都有的进行json，而cogroup则不是，只要是两个拥有的都会进行join，两者join之前自己先聚合一下
dataStream.coGroup(otherStream).where(0).equalTo(1).window(TumblingEventTimeWindows.of(Time.seconds(3))).apply {}

Connect = DataStream,DataStream → ConnectedStreams	
#"Connects" two data streams retaining their types, allowing for shared state between the two streams.
#将两个流连接起来，虽然看起来没什么用，个人感觉是为了增加一定的性能
someStream : DataStream[Int] = ...
otherStream : DataStream[String] = ...
#val connectedStreams = someStream.connect(otherStream)

CoMap, CoFlatMap = ConnectedStreams → DataStream	
Similar to map and flatMap on a connected data stream
#针对已经连接的statstream，由于schema不一致，所以使用起来应该如下示例所示
#connectedStreams.map(
#(_ : Int) => true,
#(_ : String) => false
#)
#connectedStreams.flatMap(
#(_ : Int) => true,
#(_ : String) => false
#)

#Split = DataStream → SplitStream
#Split the stream into two or more streams according to some criterion.
#有和并必然有分割，根据某种规则将其分割成多个datastream，但是有点懵的就是返回值的地方，为什么要是List[String]
#val split = someDataStream.split(
#(num: Int) =>
#(num % 2) match {
#case 0 => List("even")
#case 1 => List("odd")
#}
#)
####其实说白了就是打标签，打分组标签，之后我们从这个splitstream中取某个类型的数据的时候就通过这个list【string】来取
#Select = SplitStream → DataStream
#Select one or more streams from a split stream.
#val even = split select "even"
#val odd = split select "odd"
#val all = split.select("even","odd")

Iterate = DataStream → IterativeStream → DataStream	
#Creates a "feedback" loop in the flow, by redirecting the output of one operator to some previous operator. This is especially useful for defining algorithms that continuously update a model. The following code starts with a stream and applies the iteration body continuously. Elements that are greater than 0 are sent back to the feedback channel, and the rest of the elements are forwarded downstream. See iterations for a complete description.
#用于反馈循环，比如在训练一个模型的时候，持续不断的更新模型的参数，返回值中第一个元素是需要重新放到迭代中去的，第二个元素是该轮接续进行下轮的元素
#这个在迭代-反馈中非常实用哦
#initialStream.iterate {
#iteration => {
#val iterationBody = iteration.map {/*do something*/}
#(iterationBody.filter(_ > 0), iterationBody.filter(_ <= 0))
#}
#}

Extract Timestamps = DataStream → DataStream	
#Extracts timestamps from records in order to work with windows that use event time semantics. See Event Time.
#打时间戳，宏观上分两类，一类是数据源产生顺序的时间戳，还一类是乱序的
#一般第二种比较多，针对第二种的打时间戳也有两种方式，第一种是周期性的打时间戳，即给定延迟最大容忍时间，第二种是Punctuated，这种是每条message都打上一个水印，耗费性能
stream.assignTimestamps { timestampExtractor }



####第二部分：人工手动分区的transformation#########

Custom partitioning = DataStream → DataStream	
Uses a user-defined Partitioner to select the target task for each element.
#指定分区函数已经用来分区的字段
#dataStream.partitionCustom(partitioner, "someKey")
#dataStream.partitionCustom(partitioner, 0)

Random partitioning = DataStream → DataStream	
#Partitions elements randomly according to a uniform distribution.
#随机分区shuffle
dataStream.shuffle()

Rebalancing (Round-robin partitioning) = DataStream → DataStream	
#Partitions elements round-robin, creating equal load per partition. Useful for performance optimization in the presence of data skew.
#应用轮询调度算法分区，将数据负载均衡地分配到各个分区中
dataStream.rebalance()

Rescaling = DataStream → DataStream	
#Partitions elements, round-robin, to a subset of downstream operations. This is useful if you want to have pipelines where you, for example, fan out from each parallel instance of a source to a subset of several mappers to distribute load but don't want the full rebalance that rebalance() would incur. This would require only local data transfers instead of transferring data over network, depending on other configuration values such as the number of slots of TaskManagers.

#The subset of downstream operations to which the upstream operation sends elements depends on the degree of parallelism of both the upstream and downstream operation. For example, if the upstream operation has parallelism 2 and the downstream operation has parallelism 4, then one upstream operation would distribute elements to two downstream operations while the other upstream operation would distribute to the other two downstream operations. If, on the other hand, the downstream operation has parallelism 2 while the upstream operation has parallelism 4 then two upstream operations would distribute to one downstream operation while the other two upstream operations would distribute to the other downstream operations.

#In cases where the different parallelisms are not multiples of each other one or several downstream operations will have a differing number of inputs from upstream operations.
#与rebanlance的区别在于，rebanlance是全部重新负载均衡的分配，而rescale则是一部分进行重新分区增加或减少
dataStream.rescale()

Broadcasting = DataStream → DataStream	
Broadcasts elements to every partition.
#这里千万不要误认为是广播共享变量，这里就认为是数据在分区中的重新分配吧
dataStream.broadcast()



###第三部分：计算链 chains函数####
#其实定义计算chain的目的是调高计算性能，提高资源的利用率
Start new chain	
Begin a new chain, starting with this operator. The two mappers will be chained, and filter will not be chained to the first mapper.

someStream.filter(...).map(...).startNewChain().map(...)
Disable chaining	
Do not chain the map operator

someStream.map(...).disableChaining()
Set slot sharing group	
Set the slot sharing group of an operation. Flink will put operations with the same slot sharing group into the same slot while keeping operations that don't have the slot sharing group in other slots. This can be used to isolate slots. The slot sharing group is inherited from input operations if all input operations are in the same slot sharing group. The name of the default slot sharing group is "default", operations can explicitly be put into this group by calling slotSharingGroup("default").

someStream.filter(...).slotSharingGroup("name")